{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import re, pickle, collections, numpy as np, keras, math, operator, pdb\n",
    "\n",
    "#from gensim.models import word2vec, KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from torchvision.models import inception_v3\n",
    "import torch.nn.functional as F\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.autograd import Variable\n",
    "import re\n",
    "from os import path\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "FLOAT_DTYPE = torch.cuda.FloatTensor\n",
    "LONG_DTYPE = torch.cuda.LongTensor\n",
    "from utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dpath = 'flikr8/'\n",
    "imgpath =  'flikr8/Flicker8k_Dataset/' \n",
    "tr_img_path = imgpath +'train/'\n",
    "dev_img_path = imgpath +'dev/'\n",
    "embed_path = 'glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev_caps = pickle.load(open(dpath+'dev_first_caps.pkl', 'rb'))\n",
    "tr_caps = pickle.load(open(dpath+'train_first_caps.pkl', 'rb'))\n",
    "\n",
    "dev_caps[0], tr_caps[0]\n",
    "PAD = 0; SOS = 1; EOS = 2; UNK=3\n",
    "re_apos = re.compile(r\"(\\w)'s\\b\")         # make 's a separate word\n",
    "re_mw_punc = re.compile(r\"(\\w[’'])(\\w)\")  # other ' in a word creates 2 words\n",
    "re_punc = re.compile(\"([\\\"().,;:/_?!—])\") # add spaces around punctuation\n",
    "re_mult_space = re.compile(r\"  *\")        # replace multiple spaces with just one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_toks(sent):\n",
    "    sent = re_apos.sub(r\"\\1 's\", sent)\n",
    "    sent = re_mw_punc.sub(r\"\\1 \\2\", sent)\n",
    "    sent = re_punc.sub(r\" \\1 \", sent).replace('-', ' ')\n",
    "    sent = re_mult_space.sub(' ', sent)\n",
    "    return sent.lower().split()\n",
    "\n",
    "\n",
    "def toks2ids(sents):\n",
    "    #create new counter from all tokens in all lines\n",
    "    sents_vals = list(sents.values())\n",
    "    voc_cnt = collections.Counter(t for sent in sents_vals for t in sent) \n",
    "    \n",
    "    #sort vocab in reverse order\n",
    "    vocab = sorted(voc_cnt, key=voc_cnt.get, reverse=True)\n",
    "    \n",
    "    vocab.insert(PAD, \"<PAD>\")\n",
    "    vocab.insert(SOS, \"<SOS>\")\n",
    "    vocab.insert(EOS, \"<EOS>\")\n",
    "    vocab.insert(UNK, \"<UNK>\")\n",
    "    # {word: index of word}\n",
    "    w2id = {w:i for i,w in enumerate(vocab)}\n",
    "    #make each sentence into a list of ids\n",
    "    ids = [[w2id[t] for t in sent] for sent in sents_vals]\n",
    "    name_ids_dict = {name: id_vals for name, id_vals in zip(sents.keys(), ids)}\n",
    "    return ids, vocab, w2id, voc_cnt, name_ids_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'child',\n",
       " 'in',\n",
       " 'a',\n",
       " 'pink',\n",
       " 'dress',\n",
       " 'is',\n",
       " 'climbing',\n",
       " 'up',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'stairs',\n",
       " 'in',\n",
       " 'an',\n",
       " 'entry',\n",
       " 'way',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = {name: simple_toks(words) for name, words in tr_caps}\n",
    "dev_tokens = {name: simple_toks(words) for name, words in dev_caps}\n",
    "\n",
    "keys = list(tokens.keys()); tokens[keys[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids, vocab, w2id, counts, name_ids_dict = toks2ids(tokens)\n",
    "id2w = {v: k for k, v in w2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(ids, open(dpath+'tr_ids.pkl', 'wb'))\n",
    "pickle.dump(vocab, open(dpath+'vocab.pkl', 'wb'))\n",
    "pickle.dump(w2id, open(dpath+'w2id.pkl', 'wb'))\n",
    "pickle.dump(name_ids_dict, open(dpath+'tr_name_ids_dict.pkl', 'wb'))\n",
    "pickle.dump(id2w, open(dpath+'id2w.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createDevNameIdsDict(sents, vocab):\n",
    "    sents_vals = list(sents.values())    \n",
    "    # {word: index of word}\n",
    "    w2id = {w:i for i,w in enumerate(vocab)}\n",
    "    #make each sentence into a list of ids\n",
    "    ids = [[w2id.get(t, UNK) for t in sent] for sent in sents_vals]\n",
    "    name_ids_dict = {name: id_vals for name, id_vals in zip(sents.keys(), ids)}\n",
    "    return ids, name_ids_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_ids, dev_name_ids_dict = createValNameIdsDict(dev_tokens, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(dev_ids, open(dpath+'dev_ids.pkl', 'wb'))\n",
    "pickle.dump(dev_name_ids_dict, open(dpath+'dev_name_ids_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_path = 'glove.6B.300d.txt'\n",
    "\n",
    "def load_glove_me(loc):\n",
    "    w2v = {}\n",
    "    for line in open(loc):\n",
    "        l = line.split()\n",
    "        w2v[l[0]] = np.array(l[1:], dtype=np.float32)\n",
    "    return w2v\n",
    "\n",
    "w2v = load_glove_me(embed_path)\n",
    "n_en_vec = len(w2v.keys()) #number of words encoded\n",
    "dim_en_vec = len(w2v['the']) #dimension of embeddings -- we just use 'the' as a way to get it.\n",
    "\n",
    "w2v['king'][:10]\n",
    "\n",
    "\n",
    "\n",
    "def create_emb_mat(w2v, targ_vocab, dim_vec):\n",
    "    vocab_size = len(targ_vocab)\n",
    "    emb = np.zeros((vocab_size, dim_vec)) #initialize empty container\n",
    "    found=0\n",
    "    for i, word in enumerate(targ_vocab): \n",
    "        try: emb[i] = w2v[word]; found+=1 #if we find it, use the word's vecotrs\n",
    "        except KeyError: emb[i] = np.random.normal(scale=0.6, size=(dim_vec,)) #else randomeness\n",
    "\n",
    "    return emb, found\n",
    "\n",
    "embs, found = create_emb_mat(w2v, vocab, dim_en_vec); embs.shape, found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(embs, open(dpath+'emb_mat_300.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
