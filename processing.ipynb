{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/tensorflow/models/tree/master/research/im2txt\n",
    "- https://yashk2810.github.io/\n",
    "- https://github.com/yashk2810/Image-Captioning/blob/master/Image%20Captioning%20InceptionV3.ipynb\n",
    "- https://arxiv.org/pdf/1609.06647.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from glob import glob\n",
    "import os\n",
    "import re\n",
    "import pdb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dpath = '/data/image_captioning/flikr8/'\n",
    "img_path = dpath+'Flicker8k_Dataset/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 8091 images\n",
    "- 5 captions for each image.\n",
    "- maybe just take the first caption to start\n",
    "- 6000 train images, 1000 dev images, 1000 test images\n",
    "- will need to separate into different directories.\n",
    "- will probably want dataset captions in different files.\n",
    "- all images seem to be 500 tall\n",
    "- https://yashk2810.github.io/\n",
    "- https://github.com/yashk2810/Image-Captioning/blob/master/Image%20Captioning%20InceptionV3.ipynb\n",
    "- now you have captions. You'll need to tokenize them and embed them.\n",
    "- the next step would probably be to get the activations for the images\n",
    "- if you want to precompute them you'll need to save them in some type of specified order so you can match them with the captions.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs = glob(img_path+'/*.jpg'); len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 387)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(imgs[127]).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Caption Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, maybe just take the first caption for each image.\n",
    "- construct a regex that matches the first captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create file of only first captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match = '1003163366_44323f5815.jpg#0 '\n",
    "non_match = '1003163366_44323f5815.jpg#1 '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_cap_re = re.compile('^.+\\.jpg#0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 27), match='1003163366_44323f5815.jpg#0'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_cap.match(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_cap.match(non_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_caps_path = dpath+'Flickr8k.token.txt'\n",
    "first_caps = []\n",
    "for line in open(all_caps_path):\n",
    "    if first_cap_re.match(line):\n",
    "        line_arr = line.split('\\t')\n",
    "        img_nm = line_arr[0][:-2]\n",
    "        cap = line_arr[1].strip()\n",
    "        first_caps.append((img_nm, cap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1000268201_693b08cb0e.jpg',\n",
       " 'A child in a pink dress is climbing up a set of stairs in an entry way .')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_caps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(first_caps, open(dpath+'all_first_caps.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1000268201_693b08cb0e.jpg',\n",
       " 'A child in a pink dress is climbing up a set of stairs in an entry way .')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_caps = pickle.load(open(dpath+'all_first_caps.pkl', 'rb')); first_caps[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate train, dev and test captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getImgCapForSet(set_names_path, all_imgs_caps):\n",
    "    nms = [line.strip() for line in open(set_names_path)]\n",
    "    return [img_cap for img_cap in all_imgs_caps if img_cap[0] in nms]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_nms_caps = getImgCapForSet(dpath+'Flickr_8k.trainImages.txt', first_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_nms_caps = getImgCapForSet(dpath+'Flickr_8k.testImages.txt', first_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev_nms_caps = getImgCapForSet(dpath+'Flickr_8k.devImages.txt', first_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('1000268201_693b08cb0e.jpg',\n",
       " 'A child in a pink dress is climbing up a set of stairs in an entry way .')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tr_nms_caps)); tr_nms_caps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('1056338697_4f7d7ce270.jpg',\n",
       " 'A blond woman in a blue shirt appears to wait for a ride .')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(test_nms_caps)); test_nms_caps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('1022454332_6af2c1449a.jpg',\n",
       " 'A child and a woman are at waters edge in a big city .')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dev_nms_caps)); dev_nms_caps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(tr_nms_caps, open(dpath+'train_first_caps.pkl', 'wb'))\n",
    "pickle.dump(test_nms_caps, open(dpath+'test_first_caps.pkl', 'wb'))\n",
    "pickle.dump(dev_nms_caps, open(dpath+'dev_first_caps.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_img_names = [nm.strip() for nm in open(dpath+ 'Flickr_8k.trainImages.txt')]\n",
    "\n",
    "tr_img_path = img_path+'train'\n",
    "for nm in tr_img_names:\n",
    "    os.rename(os.path.join(img_path, nm), os.path.join(tr_img_path, nm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def moveImgs(nm_path, out_path, img_path=img_path):\n",
    "    img_names = [nm.strip() for nm in open(nm_path)]\n",
    "    for nm in img_names:\n",
    "        os.rename(os.path.join(img_path, nm), os.path.join(out_path, nm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moveImgs(dpath+ 'Flickr_8k.devImages.txt', img_path+'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moveImgs(dpath+ 'Flickr_8k.testImages.txt', img_path+'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shutil import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samp_path =img_path+'samp/wrap/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_imgs = glob(img_path+'train/wrap/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first4 = tr_imgs[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/image_captioning/flikr8/Flicker8k_Dataset/train/wrap/1082252566_8c79beef93.jpg',\n",
       " '/data/image_captioning/flikr8/Flicker8k_Dataset/train/wrap/2888408966_376c195b3f.jpg',\n",
       " '/data/image_captioning/flikr8/Flicker8k_Dataset/train/wrap/2469620360_6c620c6f35.jpg',\n",
       " '/data/image_captioning/flikr8/Flicker8k_Dataset/train/wrap/2181117039_c4eea8036e.jpg']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in first4:\n",
    "    copy(f, os.path.join(samp_path, os.path.basename(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob(samp_path+'/*.jpg'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
