{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/tensorflow/models/tree/master/research/im2txt\n",
    "- https://yashk2810.github.io/\n",
    "- https://github.com/yashk2810/Image-Captioning/blob/master/Image%20Captioning%20InceptionV3.ipynb\n",
    "- https://arxiv.org/pdf/1609.06647.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from glob import glob\n",
    "import os\n",
    "import re\n",
    "import pdb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dpath = '/data/image_captioning/flikr8/'\n",
    "dpath = 'flikr8/'\n",
    "\n",
    "img_path = dpath+'Flicker8k_Dataset/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 8091 images\n",
    "- 5 captions for each image.\n",
    "- maybe just take the first caption to start\n",
    "- 6000 train images, 1000 dev images, 1000 test images\n",
    "- will need to separate into different directories.\n",
    "- will probably want dataset captions in different files.\n",
    "- all images seem to be 500 tall\n",
    "- https://yashk2810.github.io/\n",
    "- https://github.com/yashk2810/Image-Captioning/blob/master/Image%20Captioning%20InceptionV3.ipynb\n",
    "- now you have captions. You'll need to tokenize them and embed them.\n",
    "- the next step would probably be to get the activations for the images\n",
    "- if you want to precompute them you'll need to save them in some type of specified order so you can match them with the captions.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs = glob(img_path+'/*.jpg'); len(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Caption Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, maybe just take the first caption for each image.\n",
    "- construct a regex that matches the first captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create file of only first captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match = '1003163366_44323f5815.jpg#0 '\n",
    "match_2 = '1003163366_44323f5815.jpg#2 '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_cap_re = re.compile('^.+\\.jpg#\\d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<_sre.SRE_Match object; span=(0, 27), match='1003163366_44323f5815.jpg#0'>,\n",
       " <_sre.SRE_Match object; span=(0, 27), match='1003163366_44323f5815.jpg#2'>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_cap_re.match(match), first_cap_re.match(match_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'flikr8/Flickr8k.token.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-42dab8f99876>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mall_caps_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Flickr8k.token.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mall_caps_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_caps_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfirst_cap_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mline_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'flikr8/Flickr8k.token.txt'"
     ]
    }
   ],
   "source": [
    "all_caps_path = dpath+'Flickr8k.token.txt'\n",
    "all_caps_dict = {}\n",
    "for line in open(all_caps_path):\n",
    "    if first_cap_re.match(line):\n",
    "        line_arr = line.split('\\t')\n",
    "        img_nm = line_arr[0][:-2]\n",
    "        cap = line_arr[1].strip()\n",
    "        if all_caps_dict.get(img_nm, False):\n",
    "            all_caps_dict[img_nm].append(cap)\n",
    "        else:\n",
    "            all_caps_dict[img_nm] = [cap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A black and white Border Collie catches a Frisbee in front of an audience .',\n",
       " 'a brown and white dog catches a Frisbee in it mouth in front of a group of people .',\n",
       " 'A brown and white dug jumping up to catch a Frisbee while an audience watches .',\n",
       " 'A dog jumps to catch a Frisbee , while many people watch .',\n",
       " 'An agile dog catches a Frisbee while a crowd of onlookers watches closely .']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = list(all_caps_dict.keys())\n",
    "all_caps_dict[names[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = list(all_caps_dict.values()); \n",
    "lens =[len(v) for v in values]\n",
    "min(lens), max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(all_caps_dict, open(dpath+'all_caps.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_caps = pickle.load(open(dpath+'all_caps.pkl', 'rb'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate train, dev and test captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getImgCapForSet(set_names_path, all_imgs_caps):    \n",
    "    nms = [line.strip() for line in open(set_names_path)]\n",
    "    return {k:v for k,v in all_imgs_caps.items() if k in nms} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_nms_caps = getImgCapForSet(dpath+'Flickr_8k.trainImages.txt', all_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000,\n",
       " ['Two men are sitting on the ground and going through their backpacks .',\n",
       "  'Two men sitting in front of a railing are looking at something .',\n",
       "  'Two men with backpacks placed in front of them sitting in front of a railing .',\n",
       "  'Two men with backpacks are sitting on cardboard along a railing .',\n",
       "  'Two older men with backpacks sit at a bus station .'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nms = list(tr_nms_caps.keys())\n",
    "len(tr_nms_caps), tr_nms_caps[nms[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,\n",
       " ['A man and a girl sit on the ground and eat .',\n",
       "  'A man and a little girl are sitting on a sidewalk near a blue bag eating .',\n",
       "  'A man and young girl eat a meal on a city street .',\n",
       "  'A man wearing a black shirt and a girl wearing an orange shirt sitting on the pavement eating .',\n",
       "  'A man wearing a black shirt and a little girl wearing an orange dress share a treat .'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nms_caps = getImgCapForSet(dpath+'Flickr_8k.testImages.txt', all_caps)\n",
    "nms = list(test_nms_caps.keys())\n",
    "len(test_nms_caps), test_nms_caps[nms[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,\n",
       " ['a couple with a young child wrapped in a blanket sitting on a concrete step',\n",
       "  'A woman and a man holding a child sit on steps .',\n",
       "  'A young family sits on steps enjoying the day',\n",
       "  'A young family sitting on stone steps .',\n",
       "  'The man and woman are sitting on wooden steps holding a baby .'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_nms_caps = getImgCapForSet(dpath+'Flickr_8k.devImages.txt', all_caps)\n",
    "nms = list(dev_nms_caps.keys())\n",
    "len(dev_nms_caps), dev_nms_caps[nms[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(tr_nms_caps, open(dpath+'train_all_caps.pkl', 'wb'))\n",
    "pickle.dump(test_nms_caps, open(dpath+'test_all_caps.pkl', 'wb'))\n",
    "pickle.dump(dev_nms_caps, open(dpath+'dev_all_caps.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_img_names = [nm.strip() for nm in open(dpath+ 'Flickr_8k.trainImages.txt')]\n",
    "\n",
    "tr_img_path = img_path+'train'\n",
    "for nm in tr_img_names:\n",
    "    os.rename(os.path.join(img_path, nm), os.path.join(tr_img_path, nm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def moveImgs(nm_path, out_path, img_path=img_path):\n",
    "    img_names = [nm.strip() for nm in open(nm_path)]\n",
    "    for nm in img_names:\n",
    "        os.rename(os.path.join(img_path, nm), os.path.join(out_path, nm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moveImgs(dpath+ 'Flickr_8k.devImages.txt', img_path+'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moveImgs(dpath+ 'Flickr_8k.testImages.txt', img_path+'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shutil import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samp_path =img_path+'samp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_imgs = glob(img_path+'train/wrap/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first32 = tr_imgs[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next32 = tr_imgs[32:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def copyToPath(fs, p):\n",
    "    for f in fs:\n",
    "        copy(f, os.path.join(p, os.path.basename(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "copyToPath(first32, samp_path+'train/wrap/')\n",
    "copyToPath(next32, samp_path+'val/wrap/')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
