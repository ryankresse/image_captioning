{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors\n",
    "- not masking the pad inputs for backprop\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things you're trying and may need to change back\n",
    "- batchnorm eval on cnn during training\n",
    "- tanh non-linearity when projecting cnn output to embedding space\n",
    "- using cnn output as first input to rnn instead of as hidden state -- what about SOS?\n",
    "- Train embeddings or don't use w2v?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN Mixed dnn version. The header is from one version, but we link with a different version (5103, 7003))\n",
      "/home/ubuntu/anaconda2/envs/py36/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re, pickle, collections, numpy as np, keras, math, operator, pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from torchvision.models import inception_v3\n",
    "import torch.nn.functional as F\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.autograd import Variable\n",
    "import re\n",
    "from os import path\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from data_loader import get_loader\n",
    "from build_vocab import build_vocab\n",
    "from encoder import Encoder\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from utils import *\n",
    "from create_emb_matrix import create_emb_mat\n",
    "from decoder import LSTMDecoder\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import pdb\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-988a52345229>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-988a52345229>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    - You have tuples of image names and captions.\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "- You have tuples of image names and captions. \n",
    "- Pass those into your custom dataset. Along with your transform, vocab and root directory.\n",
    "- implement the getitem function that loads and transforms the image. And also tokenizes and encodes the caption.\n",
    "- then write your collate function.\n",
    "- and get_loader function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dpath = 'flikr8/'\n",
    "imgpath =  'flikr8/Flicker8k_Dataset/' \n",
    "tr_img_path = imgpath +'train/wrap'\n",
    "dev_img_path = imgpath +'dev/wrap'\n",
    "#samp_tr_img_path = imgpath + 'samp/train/wrap/'\n",
    "MODEL_PREFIX = 'lstm_do'\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_caps = pickle.load(open(dpath+'train_first_caps.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_caps = make_samp_caps(tr_caps, samp_tr_img_path+'*.jpg'); len(samp_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/6000] Tokenized the captions.\n",
      "[1000/6000] Tokenized the captions.\n",
      "[2000/6000] Tokenized the captions.\n",
      "[3000/6000] Tokenized the captions.\n",
      "[4000/6000] Tokenized the captions.\n",
      "[5000/6000] Tokenized the captions.\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(tr_caps, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc = Encoder(batch_size=BATCH_SIZE, emb_size=300, pool_size=8, fc_in_size=2048)\n",
    "dec = LSTMDecoder(len(vocab), 300, 2)\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    enc.cuda()\n",
    "    dec.cuda()\n",
    "\n",
    "LR = 0.001\n",
    "params = list(dec.parameters()) + list(enc.linear.parameters()) + list(enc.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 avg. loss: 4.497527419879872\n",
      "epoch 1 avg. loss: 3.5567334903183805\n",
      "epoch 2 avg. loss: 3.2433970461609545\n",
      "epoch 3 avg. loss: 3.0364156743531585\n",
      "epoch 4 avg. loss: 2.8720499674479165\n",
      "epoch 5 avg. loss: 2.7310599050214215\n",
      "epoch 6 avg. loss: 2.6007933257728495\n",
      "epoch 7 avg. loss: 2.4739506219023015\n",
      "epoch 8 avg. loss: 2.353401102045531\n",
      "epoch 9 avg. loss: 2.231714392221102\n",
      "epoch 10 avg. loss: 2.1142674928070395\n",
      "epoch 11 avg. loss: 1.994261710874496\n",
      "epoch 12 avg. loss: 1.878323954920615\n",
      "epoch 13 avg. loss: 1.7615663261823757\n",
      "epoch 14 avg. loss: 1.6453179800382225\n",
      "epoch 15 avg. loss: 1.531279533140121\n",
      "epoch 16 avg. loss: 1.4222271006594422\n",
      "epoch 17 avg. loss: 1.3191927838069137\n",
      "epoch 18 avg. loss: 1.2210003432407175\n",
      "epoch 19 avg. loss: 1.1204652683709257\n",
      "epoch 20 avg. loss: 1.032291289298765\n",
      "epoch 21 avg. loss: 0.9482222526304184\n",
      "epoch 22 avg. loss: 0.8675655241935484\n",
      "epoch 23 avg. loss: 0.7928936866021925\n",
      "epoch 24 avg. loss: 0.7237726847330729\n",
      "epoch 25 avg. loss: 0.6606338665049564\n",
      "epoch 26 avg. loss: 0.6012029340190272\n",
      "epoch 27 avg. loss: 0.547460699594149\n",
      "epoch 28 avg. loss: 0.4953156337943128\n",
      "epoch 29 avg. loss: 0.4505352307391423\n",
      "epoch 30 avg. loss: 0.41310874364709343\n",
      "epoch 31 avg. loss: 0.3712605712234333\n",
      "epoch 32 avg. loss: 0.3336192100278793\n",
      "epoch 33 avg. loss: 0.30109983874905494\n",
      "epoch 34 avg. loss: 0.27280140948551956\n",
      "epoch 35 avg. loss: 0.2502998844269783\n",
      "epoch 36 avg. loss: 0.2250014889624811\n",
      "epoch 37 avg. loss: 0.20078948236280872\n",
      "epoch 38 avg. loss: 0.18124717794438844\n",
      "epoch 39 avg. loss: 0.16520419172061387\n",
      "epoch 40 avg. loss: 0.14890316993959488\n",
      "epoch 41 avg. loss: 0.14114578821325816\n",
      "epoch 42 avg. loss: 0.12742633204306325\n",
      "epoch 43 avg. loss: 0.11393120468303722\n",
      "epoch 44 avg. loss: 0.10894037062121976\n",
      "epoch 45 avg. loss: 0.0956447252663233\n",
      "epoch 46 avg. loss: 0.08821622786983367\n",
      "epoch 47 avg. loss: 0.08498743016232727\n",
      "epoch 48 avg. loss: 0.0825313598878922\n",
      "epoch 49 avg. loss: 0.07448832706738544\n",
      "epoch 50 avg. loss: 0.07149924514114216\n",
      "epoch 51 avg. loss: 0.06748534274357622\n",
      "epoch 52 avg. loss: 0.06641777612829722\n",
      "epoch 53 avg. loss: 0.05984691394272671\n",
      "epoch 54 avg. loss: 0.05888067778720651\n",
      "epoch 55 avg. loss: 0.06515668028144427\n",
      "epoch 56 avg. loss: 0.051814048520980344\n",
      "epoch 57 avg. loss: 0.048724753882295344\n",
      "epoch 58 avg. loss: 0.04293634558236727\n",
      "epoch 59 avg. loss: 0.043508452753866875\n",
      "epoch 60 avg. loss: 0.04428462571995233\n",
      "epoch 61 avg. loss: 0.05342168192709646\n",
      "epoch 62 avg. loss: 0.058974701871154125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-68:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda2/envs/py36/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda2/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda2/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda2/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/nbs/data_loader.py\", line 47, in __getitem__\n",
      "    image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
      "  File \"/home/ubuntu/anaconda2/envs/py36/lib/python3.6/site-packages/PIL/Image.py\", line 860, in convert\n",
      "    self.load()\n",
      "  File \"/home/ubuntu/anaconda2/envs/py36/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-342c2b22db8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m#dl.dataset.sanity(imgs, caps, ls)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mcaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#VOLATILE ON IMGS MAYBE?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mimgs_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nbs/utils.py\u001b[0m in \u001b[0;36mto_var\u001b[0;34m(x, volatile)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvolatile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    dl = get_loader(tr_img_path, tr_caps, vocab, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n",
    "    tot_loss = 0.0\n",
    "    for i, (imgs, caps, ls) in enumerate(dl):\n",
    "        optimizer.zero_grad()\n",
    "        #dl.dataset.sanity(imgs, caps, ls)\n",
    "        #pdb.set_trace()\n",
    "        imgs = to_var(imgs, volatile=True); caps = to_var(caps) #VOLATILE ON IMGS MAYBE?\n",
    "        targets = pack_padded_sequence(caps, ls, batch_first=True)[0]\n",
    "        imgs_enc = enc(imgs)\n",
    "\n",
    "        out = dec(imgs_enc, caps, ls)\n",
    "        loss = crit(out, targets)\n",
    "        loss.backward()\n",
    "        tot_loss += loss\n",
    "        optimizer.step()\n",
    "    torch.save(enc.state_dict(), 'models/enc_{}.pt'.format(MODEL_PREFIX))\n",
    "    torch.save(dec.state_dict(), 'models/dec_{}.pt'.format(MODEL_PREFIX))\n",
    "    torch.save(optimizer.state_dict(), 'models/opt_{}.pt'.format(MODEL_PREFIX))\n",
    "    print('epoch {} avg. loss: {}'.format(epoch, tot_loss.data[0] / i))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dl = get_loader(tr_img_path, tr_caps, vocab, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n",
    "imgs, caps = sampleBatch(enc, dec, dl)\n",
    "displaySamples(imgs, caps, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_caps = pickle.load(open(dpath+'dev_first_caps.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dl = get_loader(dev_img_path, dev_caps, vocab, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n",
    "imgs, caps = sampleBatch(enc, dec, dl)\n",
    "displaySamples(imgs, caps, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Dev Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc.load_state_dict(torch.load('models/enc_{}.pt'.format(MODEL_PREFIX)))\n",
    "dec.load_state_dict(torch.load('models/dec_{}.pt'.format(MODEL_PREFIX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def validate_dev(enc, dec, dev_caps=dev_caps, dev_img_path=dev_img_path, batch_size=BATCH_SIZE):\n",
    "    enc.eval(); dec.eval()\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    dl = get_loader(dev_img_path, dev_caps, vocab, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    \n",
    "    tot_loss = 0.0\n",
    "    for i, (imgs, caps, ls) in enumerate(dl):\n",
    "        #dl.dataset.sanity(imgs, caps, ls)\n",
    "        #pdb.set_trace()\n",
    "        imgs = to_var(imgs, volatile=True); caps = to_var(caps) #VOLATILE ON IMGS MAYBE?\n",
    "        targets = pack_padded_sequence(caps, ls, batch_first=True)[0]\n",
    "        imgs_enc = enc(imgs)\n",
    "\n",
    "        out = dec(imgs_enc, caps, ls)\n",
    "        loss = crit(out, targets)\n",
    "        tot_loss += loss\n",
    "    enc.train(); dec.train()\n",
    "    return tot_loss.data[0] / i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc = Encoder(batch_size=BATCH_SIZE, emb_size=300, pool_size=8, fc_in_size=2048)\n",
    "dec = LSTMDecoder(len(vocab), 300, 2)\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    enc.cuda()\n",
    "    dec.cuda()\n",
    "\n",
    "LR = 0.001\n",
    "params = list(dec.parameters()) + list(enc.linear.parameters()) + list(enc.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 avg. loss: 4.732233220880682\n",
      "epoch 1 avg. loss: 3.652359743169285\n",
      "epoch 2 avg. loss: 3.366281764392547\n",
      "epoch 3 avg. loss: 3.1827960498830214\n",
      "epoch 4 avg. loss: 3.038723379532921\n",
      "epoch 4 avg val loss: 3.345093282063802\n",
      "epoch 5 avg. loss: 2.932257177995488\n",
      "epoch 6 avg. loss: 2.8398555000835564\n",
      "epoch 7 avg. loss: 2.759361552682152\n",
      "epoch 8 avg. loss: 2.6880901153074865\n",
      "epoch 9 avg. loss: 2.6238199325806315\n",
      "epoch 9 avg val loss: 3.2283045450846353\n",
      "epoch 10 avg. loss: 2.5684368929123496\n",
      "epoch 11 avg. loss: 2.511904629794034\n",
      "epoch 12 avg. loss: 2.4577468627276904\n",
      "epoch 13 avg. loss: 2.412554572610294\n",
      "epoch 14 avg. loss: 2.364499341995321\n",
      "epoch 14 avg val loss: 3.230982716878255\n",
      "epoch 15 avg. loss: 2.318453538864054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-37:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda2/envs/py36/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda2/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda2/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda2/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/nbs/data_loader.py\", line 49, in __getitem__\n",
      "    image = self.transform(image)\n",
      "  File \"/home/ubuntu/anaconda2/envs/py36/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py\", line 42, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/ubuntu/anaconda2/envs/py36/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py\", line 42, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/ubuntu/anaconda2/envs/py36/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py\", line 147, in __call__\n",
      "    return F.resize(img, self.size, self.interpolation)\n",
      "  File \"/home/ubuntu/anaconda2/envs/py36/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/functional.py\", line 197, in resize\n",
      "    return img.resize((ow, oh), interpolation)\n",
      "  File \"/home/ubuntu/anaconda2/envs/py36/lib/python3.6/site-packages/PIL/Image.py\", line 1712, in resize\n",
      "    return self._new(self.im.resize(size, resample))\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ab539161df80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#dl.dataset.sanity(imgs, caps, ls)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mcaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#VOLATILE ON IMGS MAYBE?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mimgs_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nbs/utils.py\u001b[0m in \u001b[0;36mto_var\u001b[0;34m(x, volatile)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvolatile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    dl = get_loader(tr_img_path, tr_caps, vocab, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n",
    "    best_loss = 1000.0\n",
    "    tot_loss = 0.0\n",
    "    for i, (imgs, caps, ls) in enumerate(dl):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #dl.dataset.sanity(imgs, caps, ls)\n",
    "        #pdb.set_trace()\n",
    "        imgs = to_var(imgs, volatile=True); caps = to_var(caps) #VOLATILE ON IMGS MAYBE?\n",
    "        targets = pack_padded_sequence(caps, ls, batch_first=True)[0]\n",
    "        imgs_enc = enc(imgs)\n",
    "\n",
    "        out = dec(imgs_enc, caps, ls)\n",
    "        loss = crit(out, targets)\n",
    "        loss.backward()\n",
    "        tot_loss += loss\n",
    "        optimizer.step()\n",
    "    avg_tr_loss = tot_loss.data[0] / (i +1)\n",
    "    print('epoch {} avg. loss: {}'.format(epoch, avg_tr_loss))\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        avg_val_loss =  validate_dev(enc, dec)\n",
    "        print('epoch {} avg val loss: {}'.format(epoch, avg_val_loss))\n",
    "        if avg_val_loss < best_loss:\n",
    "            torch.save(enc.state_dict(), 'models/enc_{}.pt'.format(MODEL_PREFIX))\n",
    "            torch.save(dec.state_dict(), 'models/dec_{}.pt'.format(MODEL_PREFIX))\n",
    "            torch.save(optimizer.state_dict(), 'models/opt_{}.pt'.format(MODEL_PREFIX))\n",
    "            best_loss = avg_val_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
